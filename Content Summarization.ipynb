{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajaycode/machine-learning/blob/main/Content%20Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content summarization"
      ],
      "metadata": {
        "id": "7xejopr9rBuc"
      },
      "id": "7xejopr9rBuc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "reference: https://datasciencecastnet.home.blog/2022/12/30/distilhn-summarizing-news-articles-with-transformers/\n",
        "\n",
        "source: https://colab.research.google.com/drive/1WiCszznFmwdlrsozagT6IJSnuOLQUFqK?usp=sharing#scrollTo=da45ccf3"
      ],
      "metadata": {
        "id": "F0WgxnFHrgJr"
      },
      "id": "F0WgxnFHrgJr"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e821b3da",
      "metadata": {
        "id": "e821b3da",
        "outputId": "14783944-65db-4c75-8d87-91f62a5932ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 81 kB 10.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 72.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 837 kB 66.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 76.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 412 kB 68.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 293 kB 80.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 195 kB 60.4 MB/s \n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.13 which is incompatible.\n",
            "aiohttp 3.8.3 requires charset-normalizer<3.0,>=2.0, but you have charset-normalizer 3.0.1 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -qq feedparser trafilatura feedgenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trafilatura"
      ],
      "metadata": {
        "id": "hR3_4ma5tNWq",
        "outputId": "5b404ebe-8318-47d1-aa5f-277408bb63c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hR3_4ma5tNWq",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: trafilatura in /usr/local/lib/python3.8/dist-packages (1.4.0)\n",
            "Requirement already satisfied: courlan>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from trafilatura) (0.8.3)\n",
            "Requirement already satisfied: charset-normalizer>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from trafilatura) (3.0.1)\n",
            "Requirement already satisfied: htmldate>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from trafilatura) (1.4.0)\n",
            "Requirement already satisfied: justext>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from trafilatura) (3.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from trafilatura) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<2,>=1.26 in /usr/local/lib/python3.8/dist-packages (from trafilatura) (1.26.13)\n",
            "Requirement already satisfied: lxml>=4.6.4 in /usr/local/lib/python3.8/dist-packages (from trafilatura) (4.9.2)\n",
            "Requirement already satisfied: langcodes>=3.3.0 in /usr/local/lib/python3.8/dist-packages (from courlan>=0.8.3->trafilatura) (3.3.0)\n",
            "Requirement already satisfied: tld>=0.12.6 in /usr/local/lib/python3.8/dist-packages (from courlan>=0.8.3->trafilatura) (0.12.6)\n",
            "Requirement already satisfied: dateparser>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from htmldate>=1.3.2->trafilatura) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from htmldate>=1.3.2->trafilatura) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from dateparser>=1.1.2->htmldate>=1.3.2->trafilatura) (2022.6)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.8/dist-packages (from dateparser>=1.1.2->htmldate>=1.3.2->trafilatura) (1.5.1)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.8/dist-packages (from dateparser>=1.1.2->htmldate>=1.3.2->trafilatura) (2022.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->htmldate>=1.3.2->trafilatura) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f7003244",
      "metadata": {
        "id": "f7003244"
      },
      "outputs": [],
      "source": [
        "import trafilatura\n",
        "import feedparser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from feedgenerator import DefaultFeed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40115a8c",
      "metadata": {
        "id": "40115a8c"
      },
      "source": [
        "## Getting Text from a URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "da45ccf3",
      "metadata": {
        "id": "da45ccf3"
      },
      "outputs": [],
      "source": [
        "# Download an example website\n",
        "downloaded = trafilatura.fetch_url('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1bf22774",
      "metadata": {
        "id": "1bf22774",
        "outputId": "2c191619-382f-46b0-fe8a-052131450825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Every March we recognize the women who have shaped history—and now, we’re taking a look forward. From driving software development in large companies to maintaining thriving open source communities, we’re spending Women’s History Month with women leaders who are making history every day in the tech community. Erin Spiceland is a Software Engineer for SpaceX. Born and raised in rural south Georgia, she is a Choctaw and Chickasaw mother of two now living in downtown Los Angeles. Erin didn’t finish college—she’s a predominantly self-taught software engineer. In her spare time, she makes handmade Native American beadwork and regalia and attends powwows.\\nHow would you summarize your career (so far) in a single sentence?\\nMy career has been a winding road through periods of stimulation and health as well as periods of personal misery. During it all, I’ve learned a variety of programming languages and technologies while working on a diverse array of products and services. I’m a domestic abuse survivor and a Choctaw bisexual polyamorous woman. I’m so proud of myself that I made it this far considering where I came from.\\nWhat was your first job in tech like?\\nIn 2007, I had a three-year-old daughter and I was trying to finish my computer science degree one class at a time, all while keeping my house and family running smoothly. I found the math classes exciting and quickly finished my math minor, leaving only computer science classes. I was looking at about five years before I would graduate. Then, my husband at the time recommended me for an entry software developer position at a telecom and digital communications company.\\nWhen faced with the choice between an expensive computer science degree and getting paid to do what I loved, I dropped out of college and accepted the job. I was hired to work on internal tooling, and eventually, products. I did a lot of development on product front-ends, embedded network devices, and a distributed platform-as-a-service. I learned Java/JSP, Python, JavaScript/CSS, Node.js, as well as MyS'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Extract the main text (And show the start)\n",
        "trafilatura.extract(downloaded, include_comments=False, include_tables=False)[:2048]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b827cdde",
      "metadata": {
        "id": "b827cdde"
      },
      "outputs": [],
      "source": [
        "# Make a function to do this\n",
        "def get_text(url):\n",
        "    downloaded = trafilatura.fetch_url(url)\n",
        "    return trafilatura.extract(downloaded, include_comments=False, include_tables=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70f8073a",
      "metadata": {
        "id": "70f8073a"
      },
      "source": [
        "## Sumarization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See https://huggingface.co/inference-api and get your token from https://huggingface.co/settings/tokens (or visit the model page at https://huggingface.co/facebook/bart-large-cnn, click 'Deploy', choose Inference API and click 'show token to get the code ready to go)"
      ],
      "metadata": {
        "id": "Nr-eeSFS0gti"
      },
      "id": "Nr-eeSFS0gti"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdb4adff",
      "metadata": {
        "id": "fdb4adff"
      },
      "outputs": [],
      "source": [
        "API_URL = \"https://api-inference.huggingface.co/models/facebook/bart-large-cnn\"\n",
        "headers = {\"Authorization\": \"Bearer YOUR_API_KEY\"}\n",
        "\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce53d010",
      "metadata": {
        "id": "ce53d010"
      },
      "outputs": [],
      "source": [
        "def summarize(text):\n",
        "    if text is None: return None\n",
        "    output = query({\n",
        "        \"inputs\": text[:2048],\n",
        "            \"max_length\":300, \n",
        "            \"min_length\":30, \n",
        "            \"do_sample\":False\n",
        "    })\n",
        "    return output[0]['summary_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4f94ea",
      "metadata": {
        "id": "af4f94ea"
      },
      "outputs": [],
      "source": [
        "# Test this:\n",
        "text = get_text('https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/')\n",
        "summarize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "904dc40c",
      "metadata": {
        "id": "904dc40c"
      },
      "source": [
        "## Getting and Summarizing the feed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c673243",
      "metadata": {
        "id": "4c673243"
      },
      "outputs": [],
      "source": [
        "HN_Feed = feedparser.parse('https://hnrss.org/frontpage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e97b2ecb",
      "metadata": {
        "id": "e97b2ecb"
      },
      "outputs": [],
      "source": [
        "len(HN_Feed.entries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da826195",
      "metadata": {
        "id": "da826195"
      },
      "outputs": [],
      "source": [
        "# Run through the first 3 entries, getting the text and summarizing:\n",
        "for p in HN_Feed.entries[:3]:\n",
        "    summary = p['summary']\n",
        "    if 'Article URL' in summary:\n",
        "        soup = BeautifulSoup(summary, 'html.parser')\n",
        "        url = soup.find('a').get('href')\n",
        "        text = get_text(url)\n",
        "        if text is not None:\n",
        "            summary = summarize(text)\n",
        "    print(f\"Title: {p['title']}\")\n",
        "    print(f\"Summary: {summary}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87584ad",
      "metadata": {
        "id": "b87584ad"
      },
      "source": [
        "# Sharing with the World\n",
        "\n",
        "I use the `feedgenerator` package to write the summaries to a new feed (with some custom extra steps for sites like YouTube and Twitter). You can load this in your favourite feed reader by pointing it at https://www.distilhn.com/feed/feed.xml. I also made a basi index.html file that uses javascript to read this feed and insert each item into the page, visible at https://www.distilhn.com. feel free to view the page source to see how that works :)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}